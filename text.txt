<p>
<b>Basic and multi regression</b><br> 
<br>
import numpy as np<br>
import pandas as pd<br>
import matplotlib.pyplot as plt<br>
from sklearn.datasets import load_wine <br>
from sklearn import preprocessing<br>
from sklearn.model_selection import train_test_split<br>
import seaborn as sns<br>
from sklearn.linear_model import LinearRegression, HuberRegressor<br>
from sklearn.metrics import mean_absolute_error, mean_squared_error<br>
from sklearn.datasets import make_regression<br>
import numpy.random as rng<br>
import time<br>
<br>
#1.load .csv data, design matrix, output vector<br>
data=pd.read_csv("./Podaci/house_train.csv")<br>
df = pd.DataFrame(data)<br>
df = df.dropna()<br>
m = df.shape[0]<br>
X = np.ones([m, len(["sqft_living"]) + 1])<br>
X[:,1:] = preprocessing.scale(df[["sqft_living"]].to_numpy())<br>
y = preprocessing.scale(df[["price"]].to_numpy().reshape(m,1))<br>
<br>
--multi reg<br>
data=pd.read_csv("./Podaci/house_train.csv")<br>
df = pd.DataFrame(data)<br>
df = df.dropna()<br>
m = df.shape[0]<br>
X = np.ones([m, len(["sqft_living", 'bathrooms','bedrooms', 'floors']) + 1])<br>
X[:,1:] = preprocessing.scale(df[['sqft_living', 'bathrooms','bedrooms', 'floors']].to_numpy())<br>
y = preprocessing.scale(df[["price"]].to_numpy().reshape(m,1))<br>
<br>
#2. compare in out data<br>
plt.scatter(X[:,1],y)<br>

#3. determine parameters theta scikit method<br>
reg = LinearRegression(fit_intercept=True).fit(X, y)<br>
reg.coef_, reg.intercept_<br>
[theta,cost] = reg.coef_, reg.intercept_<br>
theta = theta.reshape(-1,1)<br>
reg.score(X,y)<br>
<br>
# 4. Pravac odreƒëen ùúÉnula + ùúÉjedan ùë•  show on graph<br>
plt.scatter(X[:,1], y)<br>
plt.plot(X[:,1], X.dot(theta), color="red")<br>
<br>
#5. Load test data<br>
data_test=pd.read_csv("./Podaci/house_test.csv")<br>
df_test = pd.DataFrame(data_test)<br>
df_test = df_test.dropna()<br>
m_test = df_test.shape[0]<br>
X_test = np.ones([m_test, len(["sqft_living"]) + 1])<br>
X_test[:,1:] = preprocessing.scale(df_test[["sqft_living"]].to_numpy())<br>
y_test = preprocessing.scale(df_test[["price"]].to_numpy().reshape(m_test,1))<br>
<br>
--multi reg<br>
data_test=pd.read_csv("./Podaci/house_test.csv")<br>
df_test = pd.DataFrame(data_test)<br>
df_test = df_test.dropna()<br>
m_test = df_test.shape[0]<br>
X_test = np.ones([m_test, len(["sqft_living", 'bedrooms','bathrooms', 'floors']) + 1])<br>
X_test[:,1:] = preprocessing.scale(df_test[['sqft_living', 'bedrooms','bathrooms', 'floors']].to_numpy())<br>
y_test = preprocessing.scale(df_test[["price"]].to_numpy().reshape(m_test,1))<br>
<br>
#6.Show test data and predictions for them<br>
plt.scatter(X_test[:,1], y_test)<br>
plt.plot(X_test[:,1], X_test.dot(theta), color="red")<br>
<br>
#7. test model, show error<br>
pogre≈°ka = 0<br>
y_test_prediction = X_test.dot(theta)<br>
for i,prediction in enumerate(y_test_prediction):<br>
    pogre≈°ka += (prediction -  y_test[i])**2<br>
print(pogre≈°ka/(2*X_test.shape[0]))<br>
<br>
--multi reg<br>
Y_test_pred = np.matmul(X_test,theta)
print(mean_absolute_error(y_test, Y_test_pred))<br>
print(mean_squared_error(y_test, Y_test_pred))<br>
<br>
#8. get theta from NORMAL EQUATIONS<br>
Theta_NJ = np.linalg.inv(np.matmul(X.T,X)).dot((X.T).dot(y))<br>
print(Theta_NJ)<br>
# Steps <br>
XTX = np.matmul(X.T,X)<br>
invXTX = np.linalg.inv(XTX)<br>
XTY = X.T.dot(y)<br>
Theta = invXTX.dot(XTY)<br>
print(Theta_NJ)<br>
<br>

Splitting data (csv file, test, trening data)<br>

data = pd.read_csv('./Podaci/poly_data.csv')<br>
data.head()<br>
m = data.shape[0]<br>
data_train = data.sample(round(0.8*m))<br>
data_test = data.drop(train.index)<br>

<br>
<b>Polinomial regression</b><br>
<br>
#load (ovo dolje je preko podjele jednog filea)<br>
#create design matrix and output vector<br>
m = data_train.shape[0]<br>
X = np.ones([m,4])<br>
X[:,1] = data_train.to_numpy()[:,0]<br>
X[:,2] = np.power(data_train.to_numpy()[:,0],2)<br>
X[:,3] = np.power(data_train.to_numpy()[:,0],3)<br>
Y = data_train.to_numpy()[:,1].reshape(m,1)<br>
<br>
m_test = data_test.shape[0]<br>
X_test = np.ones([m_test,4])<br>
X_test[:,1] = data_test.to_numpy()[:,0]<br>
X_test[:,2] = np.power(data_test.to_numpy()[:,0],2)<br>
X_test[:,3] = np.power(data_test.to_numpy()[:,0],3)<br>
Y_test = data_test.to_numpy()[:,1].reshape(m_test,1)<br>
<br>
#find optimal theta<br>
reg = LinearRegression().fit(X, y)<br>
theta = reg.coef_<br>
theta = theta.reshape(-1,1)<br>
<br>
#graph show<br>
def p1(x,theta):<br>
    return theta[0] + theta[1]*x + theta[2]*(x**2) + theta[3]*(x**3)<br>
plt.scatter(X[:,1], y[:])<br>
x = np.arange(0.0, 10.0, 0.01)<br>
plt.plot(x,[p1(i,theta) for i in x ], color='red')<br>
<br>
LOSS FUNKCTION (omjer gubitaka)<br>
X_b = np.array([1,2,3,4,5,6,7,8,9,10]).reshape(-1,1)<br>
y_b = np.array([1.5,10,3.6,4.7,5.6,6.8,15,8.7,9.9,10.9]).reshape(-1,1)<br>
plt.scatter(X_b,y_b)<br>
reg = LinearRegression().fit(X_b,y_b)<br>
loss_b = loss(reg.predict(y_b), y_b)<br>
print(loss_b)<br>


<dl class="field-list simple">
    <dt class="field-odd">Parameters</dt>
    <dd class="field-odd"><dl class="simple">
    <dt><strong>X</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The data to center and scale.</p>
    </dd>
    <dt><strong>axis</strong><span class="classifier">int, default=0</span></dt><dd><p>axis used to compute the means and standard deviations along. If 0,
    independently standardize each feature, otherwise (if 1) standardize
    each sample.</p>
    </dd>
    <dt><strong>with_mean</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, center the data before scaling.</p>
    </dd>
    <dt><strong>with_std</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, scale the data to unit variance (or equivalently,
    unit standard deviation).</p>
    </dd>
    <dt><strong>copy</strong><span class="classifier">bool, default=True</span></dt><dd><p>set to False to perform inplace row normalization and avoid a
    copy (if the input is already a numpy array or a scipy.sparse
    CSC matrix and if axis is 1).</p>
    </dd>
    </dl>
    </dd>
    <dt class="field-even">Returns</dt>
    <dd class="field-even"><dl class="simple">
    <dt><strong>X_tr</strong><span class="classifier">{ndarray, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The transformed data.</p>
    </dd>
    </dl>
    </dd>
    </dl>
    <div class="admonition seealso">
        <p class="admonition-title">See also</p>
        <dl class="simple">
        <dt><a class="reference internal" href="sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StandardScaler</span></code></a></dt><dd><p>Performs scaling to unit variance using the Transformer API (e.g. as part of a preprocessing <a class="reference internal" href="sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code></a>).</p>
        </dd>
        </dl>
        </div>



<br>
<b>Clasification problem</b>  <br>
<br>
import numpy as np<br>
import pandas as pd<br>
import matplotlib.pyplot as plt<br>
from sklearn import preprocessing<br>
from sklearn.datasets import make_blobs<br>
from sklearn.linear_model import Perceptron<br>
<br>
<b>Perceptron algoritam </b><br>
<br>
#0. load file data<br>
X = pd.read_csv('./Podaci/2X_a.csv').to_numpy()<br>
y = pd.read_csv('./Podaci/2y_a.csv').to_numpy()<br>
plt.scatter(X[:,0],X[:,1], c =y.reshape(-1,) )<br>
plt.show()<br>
<br>
#1. show data (sami definiramo X i y)<br>
X,y = make_blobs(n_samples=100, n_features=2, centers=2)<br>
y = np.array([-1 if i ==0 else 1 for i in y])<br>
plt.scatter(X[:,0], X[:,1], c = y)<br>
#plt.plot(np.arange(0,4), np.arange(0,4)) <br>
plt.show()<br>
<br>
#2. Model<br>
perc = Perceptron(fit_intercept = False)<br>
perc.fit(X,y)<br>
print(perc.coef_, perc.intercept_)<br>
<br>
#3. Graph<br>
x = np.arange(np.min(X)-1, np.max(X)+1)<br>
plt.scatter(X[:,0], X[:,1], c = y)<br>
plt.plot(x, (-perc.coef_[0][0]/perc.coef_[0][1])*x)<br>
plt.show()<br>
<br>
#4. Score model<br>
perc.score(X,y)<br>

<br>
#Zadan nam je model perceptrona, theta=[theta1, theta2]. Nacrtajte pravac definiran s theta. Klasificira li ispravno podatke X s y (zadani X i y)
<br>
theta2 = 3<br>
theta1 = 2<br>
theta = np.array([theta1,theta2]).reshape(-1,1)<br>
xx = np.random.uniform(-1,1,(50,1))<br>
yy = -xx*(theta1/theta2)<br>
plt.plot(xx,yy, label='pravac')<br>
plt.quiver(*[0,0], *theta, scale=5, label='theta')<br>
plt.legend()<br>
plt.gca().set_aspect('equal')  <br>
plt.show()<br>
<br>
X = np.array([(0,-0.8), (-1,0), (-0.5, -0.5), (0,1), (0.5,0.8)])<br>
y = np.array([-1, -1, -1, -1, 1])<br>
--provjeravamo jesu podaci ispravno klasificirani-usporedjujemo c i y<br>
print(np.sign(X.dot(theta)))<br>
plt.scatter(X[:,0], X[:,1], c =y)<br>
<br>
#Zadane theta=[theta1, theta2, theta0], X i y. Implementirati NAND npr<br>
theta2 = 3<br>
theta1 = 2.5<br>
theta0 = -4<br>
theta = np.array([theta1,theta2, theta0]).reshape(-1,1)<br>
# Podaci<br>
X=np.array([(0,0), (0,1), (1,0), (1,1)])<br>
y = np.array([1,1,1,0])<br>
# plot<br>
xx = np.random.uniform(-0.5,1.5,(50,1))<br>
yy = -xx*(theta1/theta2) - (theta0/theta2) <br>
plt.plot(xx,yy, color='orange', label='Pravac')<br>
plt.quiver(*[0,0], *theta, color=['purple'], scale=5, label='theta')<br>
plt.scatter(X[:,0], X[:,1], c = y)<br>
plt.legend()<br>
plt.gca().set_aspect('equal')  <br>
plt.show()<br>

<br>
Data scale<br>
# 1. Generirajmo nekoliko podataka<br>
X,_ = make_circles(50)
X = X+5
plt.scatter(X[:,0],X[:,1])
plt.gca().set_aspect('equal') 
plt.show()
# 2. Skalirajmo podatke<br>
X_scale = preprocessing.scale(X)
plt.scatter(X_scale[:,0],X_scale[:,1])
plt.gca().set_aspect('equal')  
plt.show()
# 3. ≈†to se zaista dogodilo<br>
mean = np.mean(X, axis =0)
var = np.var(X, axis=0)
X_my_scale = (X-mean)/np.sqrt(var)
plt.scatter(X_my_scale[:,0],X_my_scale[:,1])
plt.gca().set_aspect('equal')  
plt.show()

<dl class="field-list simple">
    <dt class="field-odd">Parameters</dt>
    <dd class="field-odd"><dl class="simple">
    <dt><strong>X</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The data to center and scale.</p>
    </dd>
    <dt><strong>axis</strong><span class="classifier">int, default=0</span></dt><dd><p>axis used to compute the means and standard deviations along. If 0,
    independently standardize each feature, otherwise (if 1) standardize
    each sample.</p>
    </dd>
    <dt><strong>with_mean</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, center the data before scaling.</p>
    </dd>
    <dt><strong>with_std</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, scale the data to unit variance (or equivalently,
    unit standard deviation).</p>
    </dd>
    <dt><strong>copy</strong><span class="classifier">bool, default=True</span></dt><dd><p>set to False to perform inplace row normalization and avoid a
    copy (if the input is already a numpy array or a scipy.sparse
    CSC matrix and if axis is 1).</p>
    </dd>
    </dl>
    </dd>
    <dt class="field-even">Returns</dt>
    <dd class="field-even"><dl class="simple">
    <dt><strong>X_tr</strong><span class="classifier">{ndarray, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The transformed data.</p>
    </dd>
    </dl>
    </dd>
    </dl>
    <div class="admonition seealso">
        <p class="admonition-title">See also</p>
        <dl class="simple">
        <dt><a class="reference internal" href="sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StandardScaler</span></code></a></dt><dd><p>Performs scaling to unit variance using the Transformer API (e.g. as part of a preprocessing <a class="reference internal" href="sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code></a>).</p>
        </dd>
        </dl>
        </div>



<b>SVM </b><br>
from sklearn.svm import SVC, LinearSVC<br>
import matplotlib.pyplot as plt<br>
from sklearn.datasets import make_blobs<br>
import numpy as np<br>
from sklearn.model_selection import train_test_split<br>
from sklearn.linear_model import LogisticRegression<br>
<br>
#Ako imamo zadanu geom.marginu; theta0 i trebamo pokazati samo podatke<br>
#1. create data<br>
X,y = make_blobs(n_samples = 10, n_features = 2, centers = 2, random_state=9)<br>
y = np.array([-1 if i ==0 else 1 for i in y])<br>
plt.scatter(X[:,0], X[:,1], c = y)<br>
plt.show()<br>
#2. show margin<br>
theta1 = 0.15<br>
theta2 = -0.08<br>
theta0 = 0.51<br>
theta = np.array([theta1,theta2]).reshape(-1,1)<br>
xx = np.random.uniform(-10,5,(50,1))<br>
yy = -xx*(theta1/theta2) - (theta0/theta2)<br>
plt.plot(xx,yy, color='darkblue', label='Pravac')<br>
plt.quiver(*[xx[8], yy[8]], *theta, color=['deeppink'], scale=1, label='theta')<br>
plt.scatter(X[:,0], X[:,1], c = y, cmap='PiYG')<br>
plt.legend()<br>
plt.gca().set_aspect('equal')  <br>
plt.show()<br>
<br>
<b>SVC</b> (find margin)<br>
from sklearn.datasets import make_blobs<br>
from sklearn.linear_model import LogisticRegression<br>
import numpy as np<br>
import pandas as pd<br>
import matplotlib.pyplot as plt<br>
from sklearn.svm import LinearSVC, SVC<br>
from sklearn.model_selection import train_test_split<br>
from sklearn.datasets import make_regression, make_classification<br>
from sklearn import preprocessing<br>
import seaborn as sns<br>
from sklearn.metrics import accuracy_score<br>
<br>
#1.1. Generate data and show<br>
X = np.array([[2,3.5], [2,3], [1,4], [3,3], [5,2], [4,2.25], [6,3]])<br>
y = np.array([1, 1, 1, 1, -1, -1, -1])<br>
plt.scatter(X[:,0], X[:,1], c=y)<br>
#2.Instantiate SVC model<br>
clf = SVC(kernel = 'linear')<br>
clf.fit(X, y)<br>
#3.<br>
theta = clf.coef_[0]<br>
theta_0 =  clf.intercept_[0]<br>
print("theta = ", clf.coef_, clf.intercept_)<br>
# support_vectors_ - potporni vektori
# support_ - indeksi potpornih vektora
# n_support_ - broj potpornih vektora za svaku klasu
print(clf.support_vectors_)
print("Gamma = ",np.min(np.abs((X.dot(theta)+theta_0)/np.linalg.norm(theta))) )
xx = np.linspace(1,5)<br>
yy =  -theta[0]/theta[1]* xx - theta_0 / theta[1]<br>
plt.plot(xx, yy, 'k-')<br>
plt.scatter(X[:,0], X[:,1], c=['red' if i==1 else 'yellow' for i in y])<br>
plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1],marker='*')<br>
plt.show()<br>
<br>

#2.1. Load data and show<br>
def visualize_svm(X, y, clf):<br>
 theta = clf.coef_[0]<br>
    theta_0 = clf.intercept_[0]<br>
    print(‚Äòtheta= ', clf.coef_[0], clf.intercept_[0])<br>
    #print("Potporni vektori: \n", clf.support_vectors_)<br>
    print("Gamma = ",np.min(np.abs((X.dot(theta)+theta_0)/np.linalg.norm(theta))) )<br>
    xx = np.linspace(np.min(X[:,0]),np.max(X[:,0]))<br>
    yy =  -theta[0]/theta[1]* xx - theta_0 / theta[1]<br>
    plt.plot(xx, yy, '-')<br>
    plt.scatter(X[:,0], X[:,1], c=['red' if i==1 else 'blue' for i in y]) <br>
    plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1],marker='*', color="black")<br>
    plt.show()<br>
#2. Data<br>
X = pd.read_csv('X_a.csv').to_numpy()<br>
y = pd.read_csv('y_a.csv').to_numpy().reshape(-1,)<br>
#3. Model<br>
clf = SVC(kernel = 'linear')<br>
clf.fit(X, y)<br>
#4. Call function<br>
visualize_svm(X,y, clf)<br>
<br>

#3.1. Split test and trening data from file<br>
X = pd.read_csv(data_path+'X_e.csv').to_numpy()<br>
y = pd.read_csv(data_path+'y_e.csv').to_numpy().reshape(-1,)<br>
y=2*y-1<br>
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)<br>
#2. Model<br>
clf = SVC(kernel = 'linear')<br>
clf.fit(X_train, y_train)<br>
#3.Call function<br>
visualize_svm(X_train,y_train, clf)<br>
<br>

<dl class="field-list simple">
    <dt class="field-odd">Parameters</dt>
    <dd class="field-odd"><dl class="simple">
    <dt><strong>X</strong><span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The data to center and scale.</p>
    </dd>
    <dt><strong>axis</strong><span class="classifier">int, default=0</span></dt><dd><p>axis used to compute the means and standard deviations along. If 0,
    independently standardize each feature, otherwise (if 1) standardize
    each sample.</p>
    </dd>
    <dt><strong>with_mean</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, center the data before scaling.</p>
    </dd>
    <dt><strong>with_std</strong><span class="classifier">bool, default=True</span></dt><dd><p>If True, scale the data to unit variance (or equivalently,
    unit standard deviation).</p>
    </dd>
    <dt><strong>copy</strong><span class="classifier">bool, default=True</span></dt><dd><p>set to False to perform inplace row normalization and avoid a
    copy (if the input is already a numpy array or a scipy.sparse
    CSC matrix and if axis is 1).</p>
    </dd>
    </dl>
    </dd>
    <dt class="field-even">Returns</dt>
    <dd class="field-even"><dl class="simple">
    <dt><strong>X_tr</strong><span class="classifier">{ndarray, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The transformed data.</p>
    </dd>
    </dl>
    </dd>
    </dl>
    <div class="admonition seealso">
        <p class="admonition-title">See also</p>
        <dl class="simple">
        <dt><a class="reference internal" href="sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">StandardScaler</span></code></a></dt><dd><p>Performs scaling to unit variance using the Transformer API (e.g. as part of a preprocessing <a class="reference internal" href="sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code></a>).</p>
        </dd>
        </dl>
        </div>


<b>Logistic regression</b><br>
<br>
import matplotlib.pyplot as plt<br>
from sklearn.linear_model import LogisticRegression, RidgeClassifier, LinearRegression<br>
from sklearn.datasets import make_blobs, load_digits<br>
from sklearn.metrics import accuracy_score, recall_score, precision_score<br>
from sklearn.model_selection import train_test_split<br>
from sklearn.svm import SVC<br>
<br>
#1. Define data (we have theta) show them<br>
X=np.array([[-3.78,3.96],[-3.98,3.8],[-0.95,5.18],[-2.71,4],[-0.6,4.31],[-2.58,4.65],[-2.54,4.28],[-3.55,1.62],[-3.13,4.4],[-1.93,5.62]])<br>
y = np.array([-1, -1, 1, 1, 1, 1, -1, -1, -1, 1])<br>
plt.scatter(X[:,0], X[:,1], c = y)<br>
plt.show()<br>
theta = np.array([0.325, 1.08, 0.59])<br>
<br>
#2. model function h_theta(x)<br>
def h(z):<br>
    return (1.0 / (1.0 + np.exp(-z)))<br>
    <br>
# calculate theta^T * x<br>
y_pred = h(theta[1:].dot(X[0].reshape(-1,1)) + theta[0])<br>
if y_pred>=0.5: print('predikcija 1', 'stvarna', y[0])<br>
else: print('predikcija -1', ' stvarna', y[0])<br>
<br>
# Points x,h(x)<br>
xrange = np.arange(-3, 3, 0.01)<br>
plt.plot(xrange, h(xrange))<br>
# prediction = X*theta<br>
# Points (predikcija, h(predikcija))<br>
plt.scatter(X.dot(theta[1:]) + theta[0], h(X.dot(theta[1:]) + theta[0]))<br>
plt.show()<br>
#funkcija h preslika sve na <0,1><br>
<br>
#LOSS function<br>
def J(y,yp):<br>
    return np.log(1+np.exp(-y*yp))<br>
#Stvarna vr. Je 1. Predikcije od 0 do 1, vidimo da kako je predikcija bli≈æa 1 vrijednost funkcije gubitka se smanjuje<br>
<br>
predikcije = np.arange(0.0, 1, 0.01)<br>
for i,p in enumerate(list(predikcije)):<br>
    plt.scatter(i,J(1,p))<br>
plt.show()<br>
<br>
Gotova Logistiƒçka<br>
def logistic_regression(X,y, digit):<br>
    y = np.array([1 if i ==  digit else 0 for i in y ])<br>
    log_reg = LogisticRegression(max_iter=900, solver='lbfgs').fit(X,y)<br>
    return log_reg<br>
def numbOfCorrect(y_test, y_pred):<br>
    return np.sum(np.array([y_test==y_pred]))<br>
models = []<br>
for i in range(10):<br>
    log = logistic_regression(x_train, y_train, i)<br>
    models.append(log)<br>
predictions = []<br>
for model in models:<br>
    y_pred = model.predict_proba(x_test)<br>
    predictions.append(y_pred[:,1])<br>
predictions= np.array(predictions)<br>
y_predictions = np.argmax(predictions,axis=0)<br>

</p>